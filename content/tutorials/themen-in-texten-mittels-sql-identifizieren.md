# Themen in Texten mittels SQL identifizieren

## üí° Deduktive Themenidentifikation mit SQL

Es gibt grunds√§tzlich zwei M√∂glichkeiten, Texte auf Inhalte hin zu analysieren. Der erste weg beinhaltet die **deduktive Bildung von Themenkategorien** mit entsprechenden Schlagw√∂rtern. Das bedeutet, wir √ºberlegen uns **VOR** der Betrachtung der Daten, welche Themen eine Rolle spielen k√∂nnten und welche Schlagw√∂rter zu diesen Themen geh√∂ren k√∂nnten. Unsere √úberlegungen dokumentieren wir als Tabelle mit 2 Spalten: Ein Schlagwort, wie z.B. "glyphosat", und ein von uns zugeordnetes Thema, wie z.B. "Insektensterben".

Der andere Weg ist die **induktive Bildung von Themenkategorien**. Hier identifizieren wir Schlagw√∂rter aus den Daten heraus, ordnen den Schlagw√∂rtern Themen zu, indem wir die Daten \(hier z.B.: Tweets\) sehr genau unter die Lupe nehmen, und dokumentieren das Ergebnis wieder als Tabelle mit mindestens zwei Spalten: Das Schlagwort und die Zuordnung zu einem Thema. So eine Tabelle nennen wir auch _Codebuch_.

Da wir f√ºr beide Wege iterativ umfangreiche Tabellen erstellen m√ºssen, ben√∂tigen wir einen Weg, diese zu pflegen und einfach in Databricks zu laden. Das ist die Voraussetzung, damit wir sie auf unsere Daten anwenden und so die Themen quantifizieren k√∂nnen. Ich stelle nun 2 Wege vor, wobei f√ºr die meisten F√§lle die Variante der [Google Sheets](themen-in-texten-mittels-sql-identifizieren.md#tabellen-ueber-google-sheets-pflegen-und-laden) am besten funktionieren d√ºrfte.

### üí° Einfache Erstellung einer Mapping-Tabelle mit Scala

#### Einfache Listen mit W√∂rtern oder Zahlen

Ihr k√∂nnt ganz einfach eine eigene Tabelle erstellen, die ihr sp√§ter in euren Analysen anstelle eine langen Liste wie bei `WHERE IN (...)` verwenden k√∂nnt. Dazu k√∂nnt ihr folgenden Scala-Code als Vorlage nutzen:

```scala
// Hier einfach in die W√∂rter durch eure ersetzen
val list = Seq("organic", "environment", "quality");

// Die einzige Spalte wird hier "word" genannt (gerne umbenennen)
val df = sc.parallelize(list).toDF("word");

// Zum Schluss wird ein tempor√§rer View erzeugt mit dem Namen "myTable"
df.createOrReplaceTempView("myTable");
```

Anschlie√üend k√∂nnt ihr den View wie eine Tabelle abfragen und f√ºr eure Analysen verwenden:

```sql
select * from myTable
```

#### Eine Tabelle mit 2 oder mehr Spalten

Das Beispiel oben zeigt, wie man eine Liste als Tabelle mit einer Spalte anlegen kann. In einigen F√§llen ben√∂tigt man aber 2 oder mehr Spalten, wie z.B. beim POS-Tagging. Hier braucht man eine Spalte f√ºr das Wort und eine f√ºr den Typ des Wortes \(Adjektiv, Substantiv, Nomen etc.\). Auch das geht einfach in Scala:

```scala
// Jetzt sind es Wortpaare, jeweils in eigenen Klammern mit Kommata getrennt
val list = Seq(("feed", "adjective"), ("food", "noun"), ("delicious", "adjective"))

// Nun ben√∂tigen wir auch 2 Namen f√ºr die Spalten
val df = sc.parallelize(list).toDF("word", "type")

// Und schlie√ülich erzeugen wir wieder einen View
df.createOrReplaceTempView("myPosTable")
```

Wenn wir diesen View abfragen erhalten wir im Ergebnis 2 Spalten.

Das Beispiel ist beliebig erweiterbar. Ab einer zu gro√üen Anzahl Spalten und Zeilen wird aber auch dieses Vorgehen schnell un√ºbersichtlich. Dann ergibt es mehr Sinn, die Daten auszulagern, z.B. in ein Google Spreadsheet, und dieses dann automatisiert in Databricks zu importieren.

### üí° Tabellen √ºber Google Sheets pflegen und laden

[Google Sheets](https://www.google.com/sheets/about/) ist eine kostenlose Alternative zu dem weit verbreiteten Microsoft Excel. Und weil Google Sheets in der Cloud und damit im Internet verf√ºgbar sind, lassen sie sich wunderbar in Databricks ohne komplizierte Umwege √ºber den eigenen Rechner laden. Den folgenden Code-Block m√ºsst ihr euch in euer Databricks-Notebook kopieren und in den Zeilen 2 und 6 die notwendigen Anpassungen durchf√ºhren:

```scala
// Choose a name for your resulting table in Databricks
var tableName = "stopwords"

// Replace this URL with the one from your Google Spreadsheets
// Click on File --> Publish to the Web --> Option CSV and copy the URL
var url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vSWAaX6X1mcF7iCxFXE7dvwQHxb01L4CPlwgGPkmBYDLCsHozvANJBXs_sxlEJ37tAC-jBrZ0c7ADf2/pub?output=csv"

var localpath = "/tmp/" + tableName + ".csv"
dbutils.fs.rm("file:" + localpath)
"wget -O " + localpath + " " + url !!

dbutils.fs.mkdirs("dbfs:/datasets/gsheets")
dbutils.fs.cp("file:" + localpath, "dbfs:/datasets/gsheets")

sqlContext.sql("drop table if exists " + tableName)
var df = spark.read.option("header", "true").option("inferSchema", "true").csv("/datasets/gsheets/" + tableName + ".csv");
df.write.saveAsTable(tableName);
```

In Zeile 2 ersetzt ihr einfach den Wert `"stopwords"` mit dem Namen f√ºr eure Tabelle. In Zeile 6 m√ºsst ihr nun noch die √∂ffentliche URL eures Google Sheets einf√ºgen. Wie das geht erkl√§re ich im Folgenden.

#### Ein Google Sheet als CSV ver√∂ffentlichen

Ihr legt in eurem Google Account ganz einfach ein neues Spreadsheet an und pflegt eure Daten in Spalten und Zeilen ein. Eben wie ihr es auch in Excel machen w√ºrdet. Wenn ihr dann einen Stand habt, den ihr gerne in Databricks als Tabelle verf√ºgbar laden wollt, geht ihr wie folgt vor:

**Schritt 1:** Ihr klickt auf "Datei" und dann "Im Web ver√∂ffentlichen"

![](../../.gitbook/assets/image%20%2827%29.png)

**Schritt 2:** "Gesamtes Dokument" ausw√§hlen und im rechten Dropdown-Men√º "Kommagetrennte Werte \(CSV\)" aus√§hlen.

![](../../.gitbook/assets/image%20%2816%29.png)

**Schritt 3:** Link kopieren und in Databricks einf√ºgen \(Wert in Zeile 6 ersetzen\).

![Diesen Link in die Zwischenablage kopieren.](../../.gitbook/assets/image%20%2814%29.png)

Nun m√ºsst ihr nur noch den Code-Block ausf√ºhren und anschlie√üend sollte die neue Tabelle verf√ºgbar sein. Wenn ihr anschlie√üend √Ñnderungen im Spreadsheet durchf√ºhrt und den Code zum Laden der Tabelle erneut ausf√ºhrt, habt ihr alle √Ñnderungen auch in Databricks verf√ºgbar. Das erleichtert den Prozess, gerade wenn man iterativ Tabellen erstellt, die man sehr h√§ufig in Databricks aktualisieren muss.

## üí° Induktive Themenidentifikation mit SQL

Kommt bald...

