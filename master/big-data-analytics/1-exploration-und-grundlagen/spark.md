---
description: In dieser Session lernen wir Apache Spark kennen.
---

# Spark

## â“ Fragen

* Wie funktioniert verteiltes Verarbeiten \(_distributed processing_\) mit Apache Spark?
* Welche Arten von Datenverarbeitung unterstÃ¼tzt Apache Spark?
* Welche Vorteile bietet ğŸ·Spark gegenÃ¼ber ğŸ·MapReduce?

## ğŸ· Begriffe

* ğŸ· Spark _transformations_ vs _actions_
* ğŸ· Resilient Distributed Dataset \(RDD\)
* ğŸ· Dataframe

## ğŸ”‘ Key Points

ğŸ”‘ Apache Spark reprÃ¤sentiert Daten als ğŸ·Resilient Distributed Datasets \(RDD\).

ğŸ”‘ Ein ğŸ·RDD ist eine Verkettung von Transformationen \( ğŸ·transformation\), die nacheinander ausgefÃ¼hrt werden. Ã„hnlich wie in einem Rezept. Die sequenziell ablaufenden Transformationen werden jedoch solange nicht ausgefÃ¼hrt, bis eine Aktion \( ğŸ·action\) ausgefÃ¼hrt wird. Ein Beispiel ist die Funktion `count()`, die alle DatensÃ¤tze nach Anwendung sÃ¤mtlicher Transformationen zÃ¤hlt und das Ergebnis zurÃ¼ckliefert.

ğŸ”‘ Ein RDD besitzt keinerlei Struktur, jeder Datensatz \(oder jede Zeile\) wird als Zeichenkette interpretiert. In vielen FÃ¤llen gibt es aber eine Struktur in den Quelldaten, z.B. bei CSV-Dateien. Um diese Struktur in Spark nutzbar zu machen, kÃ¶nnen ğŸ·Dataframes verwendet werden. Ein Dataframe ist somit eine Abstraktionsschicht auf einem RDD und fÃ¼gt die Information hinzu, wie aus einer Zeile Text eine Tabelle mit strukturierten Daten entsteht. Im Beispiel der CSV-Datei wÃ¤re das die einfache Regel, nach einem Trennzeichen wie Komma oder Semikolon zu suchen, und jede Zeile anhand dieser Trennzeichen in Spalten aufzuteilen.

## â–¶ Session

### 1âƒ£ Ãœbersicht Apache Spark

### 2âƒ£ Spark RDDs

### 3âƒ£ Spark Dataframes



