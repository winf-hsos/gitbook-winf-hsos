---
description: In dieser Session lernen wir Hadoop kennen.
---

# Hadoop

## â“ Fragen

* Was ist Hadoop und welche Komponenten gehÃ¶ren dazu?
* Warum ist Hadoop eine gute LÃ¶sung fÃ¼r die Herausforderungen von Big Data?
* Wie lÃ¶st HDFS das Problem, groÃŸe Datenmengen effizient und ausfallsicher zu speichern?
* Wie kÃ¶nnen mittels MapReduce sehr groÃŸe Datenmengen in kurzer Zeit verarbeitet werden?
* Welche Nachteile hat MapReduce im Vergleich zu neueren Alternativen?

## ğŸ· Begriffe

* ğŸ· HDFS
* ğŸ· MapReduce
* ğŸ· Cluster
* ğŸ· Data Node
* ğŸ· Worker Node
* ğŸ· Name Node

## ğŸ”‘ Key Points

ğŸ”‘ Hadoop ist der Name eines [Apache Open Source Projektes](https://hadoop.apache.org/), dass sich mit der Speicherung und Verarbeitung sehr groÃŸer Datenmengen \(Big Data\) befasst.

ğŸ”‘ Die wichtigsten Bestandteile von Hadoop sind das ğŸ·HDFS und das ğŸ·MapReduce Framework. Das HDFS ist ein verteiltes Dateisystem \(_distributed filesystem_\) und kÃ¼mmert sich um die Speicherung sehr groÃŸer Datenmengen. MapReduce ist ein Ansatz fÃ¼r die parallele und verteilte Verarbeitung \(_parallel and distributed processing_\) groÃŸer Datenmengen. MapReduce hÃ¤ngt von einem verteilten Dateisystem wie HDFS ab.

ğŸ”‘ HDFS und MapReduce verwenden fÃ¼r die verteilte Speicherung und die parallel Verarbeitung ein so genanntes Hadoop ğŸ·Cluster. Ein Cluster ist eine Verbund mehrerer Rechner \(Server\), die sich die Arbeitslast untereinander teilen. Einzelne Rechner in einem Cluster werden Knoten genannt \(_nodes_\). 

ğŸ”‘ In einem Hadoop Cluster werden drei Arten von Knoten unterschieden: ğŸ·Data Nodes, ğŸ·Worker Nodes und der ğŸ·Name Node. Data Nodes sind Teil des HDFS und speichern Daten. Worker Nodes sind verarbeitende Knoten im Kontext von MapReduce. Dem Name Node kommt eine zentrale Rolle zu: Nur er weiÃŸ, auf welchen Knoten welche BlÃ¶cke \(Daten\) abgelegt sind und kann auf Anfrage hin die richtigen BlÃ¶cke zur ursprÃ¼nglichen Datei zusammensetzen. Der Name Node als Verwalter sÃ¤mtlicher Metadaten ist der zentrale Zugang zum Hadoop Cluster.

ğŸ”‘ Wenn wir eine Datei im ğŸ·HDFS speichern wird sie zunÃ¤chst in BlÃ¶cke gleicher GrÃ¶ÃŸe \(im Standard 128 MB\) zerlegt und jeder Block anschlieÃŸend auf mindestens 3 Rechnern \(_nodes_\) gespeichert. Durch die redundante Speicherung jedes Blocks wird eine hohe Ausfallsicherheit gewÃ¤hrleistet. Sollte ein Knoten ausfallen, existieren noch 2 weitere Kopien.

ğŸ”‘ Wenn eine Datei aus dem ğŸ·HDFS abgerufen wird, fragt zunÃ¤chst der Name Node die notwendigen BlÃ¶cke bei den entsprechenden Rechner, auf denen sie gespeichert wurden, an. AnschlieÃŸend werden die BlÃ¶cke in der richtigen Reihenfolge zur angefragten Datei zusammengefÃ¼gt und zurÃ¼ckgeliefert.

ğŸ”‘ Von AuÃŸen betrachtet sieht das ğŸ·HDFS wie ein gewÃ¶hnliches Verzeichnissystem aus, wie man es z.B. aus dem Windows-Explorer gewohnt ist. Auch das Verhalten ist gleich. Die KomplexitÃ¤t, die im Hintergrund fÃ¼r die hohe Performanz und die Ausfallsicherheit sorgt, wird vor dem Benutzer versteckt.

## â–¶ Session

### 1âƒ£ HDFS

### 2âƒ£ MapReduce

## ğŸ”— Links

* [Webseite von Apache Hadoop](https://hadoop.apache.org/)

