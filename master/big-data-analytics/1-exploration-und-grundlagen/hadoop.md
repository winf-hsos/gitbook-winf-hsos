---
description: In dieser Session lernen wir Hadoop kennen.
---

# Hadoop, Spark & Co.

## â“ Fragen

**Apache Hadoop**

* Was ist Hadoop und welche Komponenten gehÃ¶ren dazu?
* Warum ist Hadoop eine gute LÃ¶sung fÃ¼r die Herausforderungen von Big Data?
* Wie lÃ¶st HDFS das Problem, groÃŸe Datenmengen effizient und ausfallsicher zu speichern?
* Wie funktioniert MapReduce?
* Warum kÃ¶nnen mittels MapReduce sehr groÃŸe Datenmengen in kurzer Zeit verarbeitet werden?
* Welche Nachteile hat MapReduce im Vergleich zu neueren Alternativen?
* Was sind Anwendungsbeispiele des MapReduce-Algorithmus?

**Apache Spark**

* Wie funktioniert verteiltes Verarbeiten \(_distributed processing_\) mit Apache Spark?
* Welche Arten von Datenverarbeitung unterstÃ¼tzt Apache Spark?
* Welche Vorteile bietet ğŸ·Spark gegenÃ¼ber ğŸ·MapReduce?

## ğŸ· Begriffe

* ğŸ·Verteiltes Dateisystem \(Distributed Filesystem\)
* ğŸ·Verteilte Datenverarbeitung \(Distributed Processing\)
* ğŸ·Cluster
* ğŸ·HDFS
* ğŸ·MapReduce
  * ğŸ·Data Node
  * ğŸ·Worker Node
  * ğŸ·Name Node
* ğŸ· Spark _transformations_ vs _actions_
* ğŸ· Resilient Distributed Dataset \(RDD\)
* ğŸ· Dataframe

## â–¶ Lerneinheit

### 1âƒ£ HDFS, MapReduce & Apache Spark

{% embed url="https://docs.google.com/presentation/d/1ul2cIIwSN4Ldvzq6IcoBykNM4GrnjDnwm6lqTcrTGsw/preview" %}

## ğŸ”‘ Key Points

### Apache Hadoop

* Hadoop ist der Name eines [Apache Open Source Projektes](https://hadoop.apache.org/), dass sich mit der Speicherung und Verarbeitung sehr groÃŸer Datenmengen \(Big Data\) befasst. 
* Die wichtigsten Bestandteile von Hadoop sind das ğŸ·HDFS und das ğŸ·MapReduce Framework. Das HDFS ist ein verteiltes Dateisystem \(_distributed filesystem_\) und kÃ¼mmert sich um die Speicherung sehr groÃŸer Datenmengen. MapReduce ist ein Ansatz fÃ¼r die parallele und verteilte Verarbeitung \(_parallel and distributed processing_\) groÃŸer Datenmengen. MapReduce hÃ¤ngt von einem verteilten Dateisystem wie HDFS ab. 
* HDFS und MapReduce verwenden fÃ¼r die verteilte Speicherung und die parallel Verarbeitung ein so genanntes Hadoop ğŸ·Cluster. Ein Cluster ist eine Verbund mehrerer Rechner \(Server\), die sich die Arbeitslast untereinander teilen. Einzelne Rechner in einem Cluster werden Knoten genannt \(_nodes_\).  
* In einem Hadoop Cluster werden drei Arten von Knoten unterschieden: ğŸ·Data Nodes, ğŸ·Worker Nodes und der ğŸ·Name Node. Data Nodes sind Teil des HDFS und speichern Daten. Worker Nodes sind verarbeitende Knoten im Kontext von MapReduce. Dem Name Node kommt eine zentrale Rolle zu: Nur er weiÃŸ, auf welchen Knoten welche BlÃ¶cke \(Daten\) abgelegt sind und kann auf Anfrage hin die richtigen BlÃ¶cke zur ursprÃ¼nglichen Datei zusammensetzen. Der Name Node als Verwalter sÃ¤mtlicher Metadaten ist der zentrale Zugang zum Hadoop Cluster. 
* Wenn wir eine Datei im ğŸ·HDFS speichern wird sie zunÃ¤chst in BlÃ¶cke gleicher GrÃ¶ÃŸe \(im Standard 128 MB\) zerlegt und jeder Block anschlieÃŸend auf mindestens 3 Rechnern \(_nodes_\) gespeichert. Durch die redundante Speicherung jedes Blocks wird eine hohe Ausfallsicherheit gewÃ¤hrleistet. Sollte ein Knoten ausfallen, existieren noch 2 weitere Kopien. 
* Wenn eine Datei aus dem ğŸ·HDFS abgerufen wird, fragt zunÃ¤chst der Name Node die notwendigen BlÃ¶cke bei den entsprechenden Rechner, auf denen sie gespeichert wurden, an. AnschlieÃŸend werden die BlÃ¶cke in der richtigen Reihenfolge zur angefragten Datei zusammengefÃ¼gt und zurÃ¼ckgeliefert. 
* Von AuÃŸen betrachtet sieht das ğŸ·HDFS wie ein gewÃ¶hnliches Verzeichnissystem aus, wie man es z.B. aus dem Windows-Explorer gewohnt ist. Auch das Verhalten ist gleich. Die KomplexitÃ¤t, die im Hintergrund fÃ¼r die hohe Performanz und die Ausfallsicherheit sorgt, wird vor dem Benutzer versteckt. 
* ğŸ·MapReduce ist ein Verfahren fÃ¼r die parallele Verarbeitung groÃŸer Datenmengen. Ein groÃŸer Datensatz wird dabei in kleinere Teile gesplittet, die parallel und unabhÃ¤ngig voneinander auf unterschiedlichen Rechnern \(w_orker nodes_\) im Cluster verarbeitet werden. Die Ergebnisse der Teilprozesse werden anschlieÃŸend zum Gesamtergebnis zusammengefÃ¼hrt \(_reduce_\). 
* Ein bekanntes Beispiel fÃ¼r den MapReduce-Algorithmus ist das Word Count Beispiel. Es geht darum, in einem oder vielen Texten das Vorkommen einzelner WÃ¶rter zu zÃ¤hlen und als Ergebnis eine \(sortierte\) Liste `<Wort, Anzahl>` auszugeben. HÃ¤ufig wird dies am Beispiel der Wikipedia EnzyklopÃ¤die demonstriert. MapReduce teilt den Text in kleinere Abschnitte, in denen von je einem _Worker Node_ die WÃ¶rter gezÃ¤hlt werden. Jeder _Worker Node_ liefert sein Teilergebnis zurÃ¼ck und die Teilergebnisse werden zu dem Gesamtergebnis aggregiert \(_reduce_\).

### Apache Spark

* Apache Spark reprÃ¤sentiert Daten als ğŸ·Resilient Distributed Datasets \(RDD\). 
* Ein ğŸ·RDD ist eine Verkettung von Transformationen \( ğŸ·transformation\), die nacheinander ausgefÃ¼hrt werden. Ã„hnlich wie in einem Rezept. Die sequenziell ablaufenden Transformationen werden jedoch solange nicht ausgefÃ¼hrt, bis eine Aktion \( ğŸ·action\) ausgefÃ¼hrt wird. Ein Beispiel ist die Funktion `count()`, die alle DatensÃ¤tze nach Anwendung sÃ¤mtlicher Transformationen zÃ¤hlt und das Ergebnis zurÃ¼ckliefert. 
* Ein RDD besitzt keinerlei Struktur, jeder Datensatz \(oder jede Zeile\) wird als Zeichenkette interpretiert. In vielen FÃ¤llen gibt es aber eine Struktur in den Quelldaten, z.B. bei CSV-Dateien. Um diese Struktur in Spark nutzbar zu machen, kÃ¶nnen ğŸ·Dataframes verwendet werden. Ein Dataframe ist somit eine Abstraktionsschicht auf einem RDD und fÃ¼gt die Information hinzu, wie aus einer Zeile Text eine Tabelle mit strukturierten Daten entsteht. Im Beispiel der CSV-Datei wÃ¤re das die einfache Regel, nach einem Trennzeichen wie Komma oder Semikolon zu suchen, und jede Zeile anhand dieser Trennzeichen in Spalten aufzuteilen. 
* Weil ein ğŸ·Dataframe eine Struktur besitzt \(damit meinen wir definierter Spalten und Werte\) kÃ¶nnen wir SQL verwenden, um die Daten abzufragen. Die Verwendung von SQL erÃ¶ffnet uns das Potenzial eine mÃ¤chtige Sprache fÃ¼r die Datenanalyse.

## ğŸ”— Links

* [Webseite von Apache Hadoop](https://hadoop.apache.org/)
* [Webseite von Apache Spark](https://spark.apache.org/)

