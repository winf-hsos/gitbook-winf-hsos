---
description: In dieser Session lernen wir Hadoop kennen.
---

# Hadoop, Spark & Co.

## Questions

**Apache Hadoop**

* Was ist Hadoop und welche Komponenten geh√∂ren dazu?
* Warum ist Hadoop eine gute L√∂sung f√ºr die Herausforderungen von Big Data?
* Wie l√∂st HDFS das Problem, gro√üe Datenmengen effizient und ausfallsicher zu speichern?
* Wie funktioniert MapReduce?
* Warum k√∂nnen mittels MapReduce sehr gro√üe Datenmengen in kurzer Zeit verarbeitet werden?
* Welche Nachteile hat MapReduce im Vergleich zu neueren Alternativen?
* Was sind Anwendungsbeispiele des MapReduce-Algorithmus?

**Apache Spark**

* Wie funktioniert verteiltes Verarbeiten \(_distributed processing_\) mit Apache Spark?
* Welche Arten von Datenverarbeitung unterst√ºtzt Apache Spark?
* Welche Vorteile bietet Apache Spark gegen√ºber MapReduce?

## Terms

* Verteiltes Dateisystem \(Distributed Filesystem\)
* Verteilte Datenverarbeitung \(Distributed Processing\)
* Cluster
* HDFS
* MapReduce
  * Data Node
  * Worker Node
  * Name Node
* Apache Spark
* Spark _transformations_ vs _actions_
* Resilient Distributed Dataset \(RDD\)
* Dataframe

## HDFS, MapReduce & Apache Spark

{% embed url="https://docs.google.com/presentation/d/1ul2cIIwSN4Ldvzq6IcoBykNM4GrnjDnwm6lqTcrTGsw/preview" %}

## Key Points

### Apache Hadoop

* Hadoop ist der Name eines [Apache Open Source Projektes](https://hadoop.apache.org/), dass sich mit der Speicherung und Verarbeitung sehr gro√üer Datenmengen \(**Big Data**\) befasst. 
* Die wichtigsten Bestandteile von Hadoop sind das **HDFS** und das **MapReduce** Framework. Das HDFS ist ein verteiltes Dateisystem \(_distributed filesystem_\) und k√ºmmert sich um die Speicherung sehr gro√üer Datenmengen. MapReduce ist ein Ansatz f√ºr die parallele und verteilte Verarbeitung \(_parallel and distributed processing_\) gro√üer Datenmengen. **MapReduce** h√§ngt von einem verteilten Dateisystem wie HDFS ab. 
* **HDFS** und **MapReduce** verwenden f√ºr die verteilte Speicherung und die parallele Verarbeitung ein so genanntes Hadoop **Cluster**. Ein Cluster ist eine Verbund mehrerer Rechner \(Server\), die sich die Arbeitslast untereinander teilen. Einzelne Rechner in einem Cluster werden Knoten genannt \(_nodes_\).  
* In einem Hadoop Cluster werden drei Arten von Knoten unterschieden: **Data Nodes**, **Worker Nodes** und der **Name Node**. Data Nodes sind Teil des **HDFS** und speichern Daten. Worker Nodes sind verarbeitende Knoten im Kontext von MapReduce. Dem Name Node kommt eine zentrale Rolle zu: Nur er wei√ü, auf welchen Knoten welche Bl√∂cke \(Daten\) abgelegt sind und kann auf Anfrage hin die richtigen Bl√∂cke zur urspr√ºnglichen Datei zusammensetzen. Der Name Node als Verwalter s√§mtlicher Metadaten ist der zentrale Zugang zum Hadoop **Cluster**. 
* Wenn wir eine Datei im **HDFS** speichern wird sie zun√§chst in Bl√∂cke gleicher Gr√∂√üe \(im Standard 128 MB\) zerlegt und jeder Block anschlie√üend auf mindestens 3 Rechnern \(_nodes_\) gespeichert. Durch die redundante Speicherung jedes Blocks wird eine hohe Ausfallsicherheit gew√§hrleistet. Sollte ein Knoten ausfallen, existieren noch 2 weitere Kopien. 
* Wenn eine Datei aus dem **HDFS** abgerufen wird, fragt zun√§chst der Name Node die notwendigen Bl√∂cke bei den entsprechenden Rechner, auf denen sie gespeichert wurden, an. Anschlie√üend werden die Bl√∂cke in der richtigen Reihenfolge zur angefragten Datei zusammengef√ºgt und zur√ºckgeliefert. 
* Von Au√üen betrachtet sieht das **HDFS** wie ein gew√∂hnliches Verzeichnissystem aus, wie man es z.B. aus dem Windows-Explorer gewohnt ist. Auch das Verhalten ist gleich. Die Komplexit√§t, die im Hintergrund f√ºr die hohe Performanz und die Ausfallsicherheit sorgt, wird vor dem Benutzer versteckt. 
* **MapReduce** ist ein Verfahren f√ºr die parallele Verarbeitung gro√üer Datenmengen. Ein gro√üer Datensatz wird dabei in kleinere Teile gesplittet, die parallel und unabh√§ngig voneinander auf unterschiedlichen Rechnern \(w_orker nodes_\) im Cluster verarbeitet werden. Die Ergebnisse der Teilprozesse werden anschlie√üend zum Gesamtergebnis zusammengef√ºhrt \(_reduce_\). 
* Ein bekanntes Beispiel f√ºr den MapReduce-Algorithmus ist das Word Count Beispiel. Es geht darum, in einem oder vielen Texten das Vorkommen einzelner W√∂rter zu z√§hlen und als Ergebnis eine \(sortierte\) Liste `<Wort, Anzahl>` auszugeben. H√§ufig wird dies am Beispiel der Wikipedia Enzyklop√§die demonstriert. MapReduce teilt den Text in kleinere Abschnitte, in denen von je einem _Worker Node_ die W√∂rter gez√§hlt werden. Jeder _Worker Node_ liefert sein Teilergebnis zur√ºck und die Teilergebnisse werden zu dem Gesamtergebnis aggregiert \(_reduce_\).

### Apache Spark

* Apache Spark repr√§sentiert Daten als üè∑Resilient Distributed Datasets \(RDD\). 
* Ein üè∑RDD ist eine Verkettung von Transformationen \( üè∑transformation\), die nacheinander ausgef√ºhrt werden. √Ñhnlich wie in einem Rezept. Die sequenziell ablaufenden Transformationen werden jedoch solange nicht ausgef√ºhrt, bis eine Aktion \( üè∑action\) ausgef√ºhrt wird. Ein Beispiel ist die Funktion `count()`, die alle Datens√§tze nach Anwendung s√§mtlicher Transformationen z√§hlt und das Ergebnis zur√ºckliefert. 
* Ein RDD besitzt keinerlei Struktur, jeder Datensatz \(oder jede Zeile\) wird als Zeichenkette interpretiert. In vielen F√§llen gibt es aber eine Struktur in den Quelldaten, z.B. bei CSV-Dateien. Um diese Struktur in Spark nutzbar zu machen, k√∂nnen üè∑Dataframes verwendet werden. Ein Dataframe ist somit eine Abstraktionsschicht auf einem RDD und f√ºgt die Information hinzu, wie aus einer Zeile Text eine Tabelle mit strukturierten Daten entsteht. Im Beispiel der CSV-Datei w√§re das die einfache Regel, nach einem Trennzeichen wie Komma oder Semikolon zu suchen, und jede Zeile anhand dieser Trennzeichen in Spalten aufzuteilen. 
* Weil ein üè∑Dataframe eine Struktur besitzt \(damit meinen wir definierter Spalten und Werte\) k√∂nnen wir SQL verwenden, um die Daten abzufragen. Die Verwendung von SQL er√∂ffnet uns das Potenzial eine m√§chtige Sprache f√ºr die Datenanalyse.

## Links

* [Webseite von Apache Hadoop](https://hadoop.apache.org/)
* [Webseite von Apache Spark](https://spark.apache.org/)

